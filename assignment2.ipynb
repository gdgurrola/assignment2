{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling and Exploratory Data Analysis\n",
        "## Do Q1 and Q2, and one other question.\n",
        "`! git clone https://www.github.com/DS3001/assignment2`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "28fec859",
      "metadata": {},
      "source": [
        "1. This paper is about tidying data and goes over how to make cleaning data simple and easy. The paper goes over tools for making it easier to tidy data and then use it for data analysis that consist of making data strucutes uniform and consistent and going over data manipulation. \n",
        "2. The \"tidy data standard\" makes the first steps of data cleaning and preparing easier because it means you do not need to start from square one and do something brand new every time you start cleaning your data. The tidy data standards intends to simplify the the beginning stages of data analysis and make the anaysis easier so that you can focus on the patterns and observations rather than trying to understand the logistics and cleaning of the data. \n",
        "3. The sentence \"Like families, tidy datasets are all alike, but every messy dataset is messy in its own way\" means that the tiday datasets all follow the tidy data standard which gives them similar characteristics because they are all initially cleaned up using the same data strucrtures and prepared so that they can be better understood during data analysis. Datasets that do not follow the tidy data standard are all unique because they each have missing values, outliers, and other inconsistencies that are distinct for that specific dataset, which is what the second half of the sentence from the introduction is converying. The sentence \"For a given dataset, it's ususally easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\" is conveying that when doing analysis on one specific dataset, it is simple to identify what the observations and variables are based on how they are organized in the data structure, however the paper displays the same data in 3 differnt tables, which can make it confusing and more complicated to identify what are the observations and what are the variables because in a broader, more general context, the data can be distributed in an unstructured way which can make it more complicated to establish what are the data observations and variables. \n",
        "4. values - what a dataset is made up of. values can either be quantitative as numbers or qualitative as a string. Every value belongs to a variable and an observation  \n",
        "     variable - a variable contains all values that measure the same attribute (ex: height, temperature, duration). It is easier to describe funcitonal relationships between variables  \n",
        "     observation - contains all values measured on the same unit (person, day, or race). It is easier to make comparisons between groups of observations \n",
        "5. There are 3 attributes of tidy data. In tidy data it is required that each variable forms a column, each observation forms a row, and each type of observational unit forms a table. This is a standard way of mapping the meaning of a dataset to its datastructure. \n",
        "6. The five most common problems with messy data sets are:  \n",
        "    + Column headers are values, not variable names\n",
        "    + Multiple variables are stored in one column\n",
        "    + Variables are stored in both rows and columns\n",
        "    + Multiple types of observational units are stored in the same table\n",
        "    + A single observational unit is stored in multiple tables \n",
        "    The data in Table 4 are messy because variables are being stored in both the rows and the columns and the column headers are values not names. To tidy this data, we can melt the data. Data melting is when you turn rows into columns and reshape the data into a long format. \n",
        "7. Table 11 is messy data because the dates are stored as separate variables which leaves a large amount of blank values in the table. Table 12 is molten because it combines the columns from table 11 so that there is 1 variable that represents the date, and this makes the dataset more narrow, and the tidy data set, there are two measured variables so that there are less entries in the table, and the observations are more concise, rather than having two observations for each tmax and tmin. \n",
        "8. The chicken-and-egg problem that is present with tidy data is that if tidy data is only as useful as the tools that work with it, so the availability and development of tools will only be produced and used if tidy data continues to be implemented and utilized as well. Wickham hopes that others in the future will continue to build on tidy data framework to develop better solutions for data storage and even better tools to tackle the challenge of tidying data for better data anaysis. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "12ae1bed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float64\n",
            "0    145.0\n",
            "1     37.0\n",
            "2     28.0\n",
            "3    199.0\n",
            "4    549.0\n",
            "5    149.0\n",
            "6    250.0\n",
            "7     90.0\n",
            "8    270.0\n",
            "9    290.0\n",
            "Name: Price, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Question 2.1 - Cleaning Price variable in airbnb dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('./data/airbnb_hw.csv')\n",
        "price = 'Price'\n",
        "df[price] = df[price].str.replace(',','') # deleting commas \n",
        "df[price] = df[price].replace(' ',np.nan) # Notice the column replacement\n",
        "df[price] = df[price].astype(float).round(2)\n",
        "print(df[price].dtype)\n",
        "print(df[price].head(10))\n",
        "\n",
        "\n",
        "# In order to clean the Price variable, the first thing that I did was delete all of the commas that existed in the entries for\n",
        "# prices that were greater than 1,000. By deleting the commas, I was then able to convert the data type from an object so that \n",
        "# all of these data were uniform as a float, and were rounded to two decimal places since they represent the prices in dollars. Before\n",
        "# doing all of this cleaning, the datatype was not uniform and there were inconsistencies in the data because values over 1000 had commas, \n",
        "# so it was not as easy to parse through to find certain values. Now that the data is clean, analysis can be done because it can be sorted through \n",
        "# and filtered because they are all quantitative numbers of type float where before, the type did not allow for ease of filtering and analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "ccc68549",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 'Type' variables before cleaning:  ['Unprovoked' 'Provoked' 'Questionable' 'Watercraft' 'Unconfirmed'\n",
            " 'Unverified' 'Invalid' 'Under investigation' 'Boating' 'Sea Disaster' nan\n",
            " 'Boat' 'Boatomg']\n",
            "['Unprovoked' 'Provoked' 'Unknown' 'Watercraft' 'Sea Disaster']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/8_/6hshqn4n5xbfjbj0cjjrrv9w0000gn/T/ipykernel_43539/2811870783.py:2: DtypeWarning: Columns (10,17,18,19,20,21,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  sharks_df = pd.read_csv('./data/sharks.csv')\n"
          ]
        }
      ],
      "source": [
        "# Question 2.2 - Cleaning Type variable in shark dataset \n",
        "sharks_df = pd.read_csv('./data/sharks.csv')\n",
        "var = 'Type' \n",
        "unique_types = sharks_df[var].unique()\n",
        "print(\"Type variables before cleaning: \" , unique_types) \n",
        " # dropping the observations with no value\n",
        "sharks_df.dropna(subset=[var], inplace=True)\n",
        "\n",
        "# Combining some of the types because they are similar \n",
        "sharks_df[var] = sharks_df[var].replace(['Boating', 'Boat', 'boatomg', 'Watercraft', 'Boatomg'], 'Watercraft')\n",
        "sharks_df[var] = sharks_df[var].replace(['Unconfirmed', 'Unverified', 'Questionable', 'Under investigation', 'Invalid'], 'Unknown')\n",
        "\n",
        "unique_types = sharks_df[var].unique()\n",
        "print('Type variables after cleaning: ' , unique_types) \n",
        "\n",
        "# In order to clean up the Type variable in the sharks dataset, the first thing That I did was see what the values were that existed for \n",
        "# observations in the data frame. There were some observations that had nan as their entry, and after searching in the data set it looked \n",
        "# like lots of these nan were empty observations that did not have other data, so I dropped them from the dataframe. The next thing that I did \n",
        "# to clean the data was replace the incorrect spelling such as 'boatomg' and 'Boatomg' and also combined certain values if they indicated the same \n",
        "# result. This cleaned the data by eliminating inconsistencies in the values and consolidating the values that exist for that variable if they \n",
        "# were representing the same outcome before. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "8de3958e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question 3 - Dummy Variable \n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "pretrial_df = pd.read_csv(url,low_memory=False) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "72b6ca43",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1    19154\n",
            "0     3801\n",
            "9       31\n",
            "Name: WhetherDefendantWasReleasedPretrial, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "var = 'WhetherDefendantWasReleasedPretrial'\n",
        "print(pretrial_df[var].value_counts())\n",
        "\n",
        "pretrial_df[var] = pretrial_df[var].replace(9, np.nan)\n",
        "\n",
        "\n",
        "# This is a dummy variable which means it is representing a binary variable, 1s indicate that the defendant was released pretrial \n",
        "# and 0s mean that the defendant was not released pretrial. In order to find what the values were for this variable, I printed the unique values and found out \n",
        "# that the values were [9 0 1], and there were 31 observations with 9 as the value. The 9s that exist for observations were making the data very messy, \n",
        "# so in order to clean the data I replaced all of the 9s with np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "043b423c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    9053\n",
            "0                   4953\n",
            "12                  1404\n",
            ".985626283367556    1051\n",
            "6                    809\n",
            "                    ... \n",
            "49.9712525667351       1\n",
            "57.0349075975359       1\n",
            "79.9260780287474       1\n",
            "42.1642710472279       1\n",
            "1.6570841889117        1\n",
            "Name: ImposedSentenceAllChargeInContactEvent, Length: 484, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImposedSentenceAllChargeInContactEvent</th>\n",
              "      <th>SentenceTypeAllChargesAtConvictionInContactEvent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.985626283367556</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.91375770020534</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td></td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td></td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>120</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ImposedSentenceAllChargeInContactEvent  \\\n",
              "0                                           \n",
              "1                                      60   \n",
              "2                                      12   \n",
              "3                        .985626283367556   \n",
              "4                                           \n",
              "5                                      12   \n",
              "6                                      36   \n",
              "7                                       6   \n",
              "8                                      24   \n",
              "9                                      12   \n",
              "10                       5.91375770020534   \n",
              "11                                          \n",
              "12                                          \n",
              "13                                    120   \n",
              "14                                     24   \n",
              "\n",
              "    SentenceTypeAllChargesAtConvictionInContactEvent  \n",
              "0                                                  9  \n",
              "1                                                  0  \n",
              "2                                                  1  \n",
              "3                                                  1  \n",
              "4                                                  4  \n",
              "5                                                  0  \n",
              "6                                                  1  \n",
              "7                                                  0  \n",
              "8                                                  1  \n",
              "9                                                  1  \n",
              "10                                                 0  \n",
              "11                                                 4  \n",
              "12                                                 4  \n",
              "13                                                 1  \n",
              "14                                                 1  "
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 2.4 Missing values, not at random: For the pretrial data covered in the lecture, \n",
        "# clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. \n",
        "# (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)\n",
        "\n",
        "var1 = 'ImposedSentenceAllChargeInContactEvent'\n",
        "var2 = 'SentenceTypeAllChargesAtConvictionInContactEvent'\n",
        "new_df = pretrial_df[[var1,var2]]\n",
        "\n",
        "print(pretrial_df[var1].value_counts())\n",
        "new_df.head(15)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c11bcd96-2834-41a4-80fe-d354b4277fd9",
      "metadata": {
        "id": "c11bcd96-2834-41a4-80fe-d354b4277fd9"
      },
      "source": [
        "**Q3.** This question provides some practice doing exploratory data analysis and visualization.\n",
        "\n",
        "The \"relevant\" variables for this question are:\n",
        "  - `level` - Level of institution (4-year, 2-year)\n",
        "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
        "  - `control` - Public, Private not-for-profit, Private for-profit\n",
        "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
        "\n",
        "1. Load the `./data/college_completion.csv` data with Pandas.\n",
        "2. What are are the dimensions of the data? How many observations are there? What are the variables included? Use `.head()` to examine the first few rows of data.\n",
        "3. Cross tabulate `control` and `level`. Describe the patterns you see.\n",
        "4. For `grad_100_value`, create a histogram, kernel density plot, boxplot, and statistical description.\n",
        "5. For `grad_100_value`, create a grouped kernel density plot by `control` and by `level`. Describe what you see. Use `groupby` and `.describe` to make grouped calculations of statistical descriptions of `grad_100_value` by `level` and `control`. Which institutions appear to have the best graduation rates?\n",
        "6. Create a new variable, `df['levelXcontrol']=df['level']+', '+df['control']` that interacts level and control. Make a grouped kernel density plot. Which institutions appear to have the best graduation rates?\n",
        "7. Make a kernel density plot of `aid_value`. Notice that your graph is \"bi-modal\", having two little peaks that represent locally most common values. Now group your graph by `level` and `control`. What explains the bi-modal nature of the graph? Use `groupby` and `.describe` to make grouped calculations of statistical descriptions of `aid_value` by `level` and `control`.\n",
        "8. Make a scatterplot of `grad_100_value` by `aid_value`. Describe what you see. Now make the same plot, grouping by `level` and then `control`. Describe what you see. For which kinds of institutions does aid seem to increase graduation rates?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "98d34a3b-c21d-4dc9-a8d2-fb7686804ceb",
      "metadata": {
        "id": "98d34a3b-c21d-4dc9-a8d2-fb7686804ceb"
      },
      "source": [
        "**Q4.** This question uses the Airbnb data to practice making visualizations.\n",
        "\n",
        "  1. Load the `./data/airbnb_hw.csv` data with Pandas. You should have cleaned the `Price` variable in question 2, and you'll need it later for this question.\n",
        "  2. What are are the dimensions of the data? How many observations are there? What are the variables included? Use `.head()` to examine the first few rows of data.\n",
        "  3. Cross tabulate `Room Type` and `Property Type`. What patterns do you see in what kinds of rentals are available? For which kinds of properties are private rooms more common than renting the entire property?\n",
        "  4. For `Price`, make a histogram, kernel density, box plot, and a statistical description of the variable. Are the data badly scaled? Are there many outliers? Use `log` to transform price into a new variable, `price_log`, and take these steps again.\n",
        "  5. Make a scatterplot of `price_log` and `Beds`. Describe what you see. Use `.groupby()` to compute a desciption of `Price` conditional on/grouped by the number of beds. Describe any patterns you see in the average price and standard deviation in prices.\n",
        "  6. Make a scatterplot of `price_log` and `Beds`, but color the graph by `Room Type` and `Property Type`. What patterns do you see? Compute a description of `Price` conditional on `Room Type` and `Property Type`. Which Room Type and Property Type have the highest prices on average? Which have the highest standard deviation? Does the mean or median appear to be a more reliable estimate of central tendency, and explain why?\n",
        "  7. We've looked a bit at this `price_log` and `Beds` scatterplot. Use seaborn to make a `jointplot` with `kind=hex`. Where are the data actually distributed? How does it affect the way you think about the plots in 5 and 6?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "bb0b1378",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "air bnb dataframe dimensions:  (30478, 13)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Host Id</th>\n",
              "      <th>Host Since</th>\n",
              "      <th>Name</th>\n",
              "      <th>Neighbourhood</th>\n",
              "      <th>Property Type</th>\n",
              "      <th>Review Scores Rating (bin)</th>\n",
              "      <th>Room Type</th>\n",
              "      <th>Zipcode</th>\n",
              "      <th>Beds</th>\n",
              "      <th>Number of Records</th>\n",
              "      <th>Number Of Reviews</th>\n",
              "      <th>Price</th>\n",
              "      <th>Review Scores Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5162530</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 Bedroom in Prime Williamsburg</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Entire home/apt</td>\n",
              "      <td>11249.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33134899</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sunny, Private room in Bushwick</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Private room</td>\n",
              "      <td>11206.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>37.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39608626</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sunny Room in Harlem</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Private room</td>\n",
              "      <td>10032.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>28.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>500</td>\n",
              "      <td>6/26/2008</td>\n",
              "      <td>Gorgeous 1 BR with Private Balcony</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Entire home/apt</td>\n",
              "      <td>10024.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>500</td>\n",
              "      <td>6/26/2008</td>\n",
              "      <td>Trendy Times Square Loft</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>95.0</td>\n",
              "      <td>Private room</td>\n",
              "      <td>10036.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>549.0</td>\n",
              "      <td>96.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Host Id Host Since                                Name Neighbourhood   \\\n",
              "0   5162530        NaN     1 Bedroom in Prime Williamsburg       Brooklyn   \n",
              "1  33134899        NaN     Sunny, Private room in Bushwick       Brooklyn   \n",
              "2  39608626        NaN                Sunny Room in Harlem      Manhattan   \n",
              "3       500  6/26/2008  Gorgeous 1 BR with Private Balcony      Manhattan   \n",
              "4       500  6/26/2008            Trendy Times Square Loft      Manhattan   \n",
              "\n",
              "  Property Type  Review Scores Rating (bin)        Room Type  Zipcode  Beds  \\\n",
              "0     Apartment                         NaN  Entire home/apt  11249.0   1.0   \n",
              "1     Apartment                         NaN     Private room  11206.0   1.0   \n",
              "2     Apartment                         NaN     Private room  10032.0   1.0   \n",
              "3     Apartment                         NaN  Entire home/apt  10024.0   3.0   \n",
              "4     Apartment                        95.0     Private room  10036.0   3.0   \n",
              "\n",
              "   Number of Records  Number Of Reviews  Price  Review Scores Rating  \n",
              "0                  1                  0  145.0                   NaN  \n",
              "1                  1                  1   37.0                   NaN  \n",
              "2                  1                  1   28.0                   NaN  \n",
              "3                  1                  0  199.0                   NaN  \n",
              "4                  1                 39  549.0                  96.0  "
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 4.2\n",
        "print(\"air bnb dataframe dimensions: \" , df.shape)\n",
        "# There are 30478 observations in in the dataframe, and 13 variables included \n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "de896ae3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Property Type    Apartment  Bed & Breakfast  Boat  Bungalow  Cabin  Camper/RV  \\\n",
            "Room Type                                                                       \n",
            "Entire home/apt      15669               13     7         4      1          6   \n",
            "Private room         10748              155     1         0      1          1   \n",
            "Shared room            685               12     0         0      0          0   \n",
            "\n",
            "Property Type    Castle  Chalet  Condominium  Dorm  House  Hut  Lighthouse  \\\n",
            "Room Type                                                                    \n",
            "Entire home/apt       0       0           72     4    752    0           1   \n",
            "Private room          1       1           22    16   1258    2           0   \n",
            "Shared room           0       0            0    11     80    0           0   \n",
            "\n",
            "Property Type    Loft  Other  Tent  Townhouse  Treehouse  Villa  \n",
            "Room Type                                                        \n",
            "Entire home/apt   392     14     0         83          0      4  \n",
            "Private room      312     29     4         52          1      4  \n",
            "Shared room        49      4     0          1          3      0  \n"
          ]
        }
      ],
      "source": [
        "# Question 4.3: Cross tabulate `Room Type` and `Property Type`. \n",
        "# What patterns do you see in what kinds of rentals are available? \n",
        "# For which kinds of properties are private rooms more common than renting the entire property?\n",
        "\n",
        "data = pd.crosstab(df['Room Type'], df['Property Type'])\n",
        "print(data)\n",
        "\n",
        "# The most common type of rental that is available are apartments, and when there are apartments available, there\n",
        "# are varieties between renting the entire apartment or renting a private room. Another popular property type is a loft, bed and breakfast, \n",
        "# and condominium. The least popular room type is shared room, lots of the property types do not have this as an option to rent out. \n",
        "#   Private rooms are more common than renting the entire property in Houses. There are 1258 available private rooms in houses\n",
        "# but only 752 entire homes available. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4120e66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question 4.4: For `Price`, make a histogram, kernel density, box plot, and a statistical description of the variable. \n",
        "# Are the data badly scaled? Are there many outliers?\n",
        "# Use `log` to transform price into a new variable, `price_log`, and take these steps again.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q5.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2f38f2fd-6381-481d-bba9-017f3d363426",
      "metadata": {
        "id": "2f38f2fd-6381-481d-bba9-017f3d363426"
      },
      "source": [
        "**Q6.** Open the `./data/CBO_data.pdf` file. This contains tax data for 2019, explaining where the money comes from that the U.S. Federal Government Spends in terms of taxation on individuals/families and payroll taxes (the amount that your employer pays in taxes on your wages).\n",
        "\n",
        "For some context, the Federal government ultimately spent about $4.4 trillion in 2019, which was 21% of GDP (the total monetary value of all goods and services produced within the United States). Individual Income Taxes is the amount individuals pay on their wages to the Federal government, Corporate Income Taxes is the taxes individuals pay on capital gains from investment when they sell stock or other financial instruments, Payroll Taxes is the tax your employer pays on your wages, Excises and Customs Duties are taxes on goods or services like sin taxes on cigarettes or alcohol, and Estate and Gift Taxes are taxes paid on transfers of wealth to other people.\n",
        "\n",
        "1. Get the Millions of Families and Billions of Dollars data into a .csv file and load it with Pandas.\n",
        "2. Create a bar plot of individual income taxes by income decile. Explain what the graph shows. Why are some values negative?\n",
        "3. Create a bar plot of Total Federal Taxes by income decile. Which deciles are paying net positive amounts, and which are paying net negative amounts?\n",
        "4. Create a stacked bar plot for which Total Federal Taxes is grouped by Individual Income Taxes, Payroll Taxes, Excises and Customs Duties, and Estate and Gift Taxes. How does the share of taxes paid vary across the adjusted income deciles? (Hint: Are these the kind of data you want to melt?)\n",
        "5. Below the Total line for Millions of Families and Billions of Dollars, there are data for the richest of the richest families. Plot this alongside the bars for the deciles above the Total line. Describe your results.\n",
        "6. Get the Percent Distribution data into a .csv file and load it with Pandas. Create a bar graph of Total Federal Taxes by income decile.\n",
        "7. A tax system is progressive if higher-income and wealthier individuals pay more than lower-income and less wealthy individuals, and it is regressive if the opposite is true. Is the U.S. tax system progressive in terms of amount paid? In terms of the percentage of the overall total?\n",
        "8. Do the rich pay enough in taxes? Defend your answer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
